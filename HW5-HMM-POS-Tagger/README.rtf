{\rtf1\ansi\ansicpg1252\cocoartf2636
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww20840\viewh15040\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs36 \cf0 hmm_model_build.py
\fs24 \
\
1. I am using pickle module to open brown_tagged_sents.dat\
2. I am dumping transition, emission, vocabulary(to handle unknown words) and unigram tags (for all my states of tags for Viterbi) into my model.dat\
3. A, B are dictionaries of tuples and all calculations are logged <<<<\
4. vocabulary and my tags are all using dictionary structure. \
5. I am keeping tags as they are and not stripping off anything after \'93-\'93  <<<<\
\
\

\fs36 hmm_sequence.py
\fs24 \
\
1. Input format: python3 hmm_sequence.py model.dat example.txt\
2. To handle unknown words you have two options: 1. using words that have appeared in the corpus once 2. Using same probability over all tags (1/474)\
By default it is using words that have occurred once to handle unknown words\
3. Since calculations in A & B matrices are all in log format, I am adding them together instead of multiplying\
\
\

\fs36 Hmm_viterbi.py
\fs24 \
\
1. Input format the same as hmm_sequnce.py\
2. Same hmm_sequence.py approach for handling unknown words \
3. My Viterbi algorithm is slightly different than book\'92s pseudo code.\
4. Viterbi is a list of dictionaries. Length of this list is equal to length of untagged words from input.\
Inside each dictionary, keys are states (tags) and each value is again a dictionary of probability and previous state. \
4. Backtracking is using the previous value of Viterbi keys.\
5. Viterbi function is returning a list of tags which should be parsed and combined with word sequence again.\
6. Since every pre-calculations are logged, I am adding them instead of multiplying}